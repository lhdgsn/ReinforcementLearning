{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Cart-Pole tutorial\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion() # turn on interactive mode\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on rendering (specific to Jupyter Notebook)\n",
    "* env.render(mode='rgb_array') only works with pyglet v1.2.4 ($pip install pyglet==1.2.4) and not newer versions\n",
    "\n",
    "* to prevent render from opening in a new window, open python notebook using xvfb ($xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "observation = env.reset()\n",
    "\n",
    "fig = plt.figure('Cart pole')\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "for _ in range(200):\n",
    "    x = env.render(mode='rgb_array') # in rgb_array mode this returns a numpy array\n",
    "    \n",
    "    # update the figure to show new state\n",
    "    img.set_data(x) # this is a lot faster than calling imshow every time\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if(done): # it looks like the failure condition is quite a small deviation from vertical (it's 12deg)\n",
    "        break\n",
    "\n",
    "# # alternatively\n",
    "# fig = plt.figure()\n",
    "# img = plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "#            interpolation='none')\n",
    "\n",
    "# for i in range(500):\n",
    "#     # update the figure to show new state\n",
    "#     img.set_data(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
    "#     fig.canvas.draw()\n",
    "\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, info = env.step(action)\n",
    "#     if(done):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory\n",
    "Stores transitions. randomly sampling these decorrelates them (stabilizes, improves training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'new_state', 'reward'])\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # circular-buffer-like, with length of list expanding up to capacity, and then overwriting oldest entries\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        '''Adds a transition to memory'''\n",
    "        # if memory hasn't stored max number of states, expand size\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        # store transition in memory. if memory is at capacity, overwrite oldest stored transition\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        # increment position in memory queue\n",
    "        self.position = (self.position + 1) % self.capacity \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''Returns batch_size number of randomly selected stored transitions'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    # overload len()\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN algorithm\n",
    "note: environment is deterministic.\n",
    "\n",
    "Want to train a policy $\\pi(s)$ (mapping from state to action) that maximizes the discounted return $R_t = \\Sigma_{t}^{T}\\gamma^t r_t$, where $\\gamma \\in [0,1]$ is the discount factor, and $r_t$ is the reward received at time $t$\n",
    "\n",
    "Want to find an optimal value function $Q^*(s,a)$ which quantifies the value (expected return) of taking action $a$ from state $s$. It is then easy to find the optimal policy $\\pi^*(s) = argmax_aQ^*(s,a)$, i.e. to always choose the action with maximum expected return.\n",
    "\n",
    "Approximate $Q^*$ using a neural network.\n",
    "\n",
    "The value function $Q$ obeys the deterministic Bellman equation $Q^{\\pi}(s,a) = r + \\gamma Q^{\\pi}(s',\\pi(s))$, i.e. the state-action value is equal to the reward received from taking action $a$ plus the discounted value of reaching the new state $s'$\n",
    "\n",
    "We want $Q^{\\pi}$ to converge to $Q^*$, so the loss is defined using the temporal difference error $\\delta = Q^{\\pi}(s,a) - (r + \\gamma max_a Q^{\\pi}(s',a))$\n",
    "\n",
    "Use Huber loss over batch $B$ sampled from replay memory: $L = \\frac{1}{|B|}\\Sigma L(\\delta)$ where $L{\\delta} = \\frac{1}{2}\\delta^2$ if $|\\delta| < 1$, $|\\delta| - \\frac{1}{2}$ otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-network\n",
    "Train CNN that takes difference between current and previous frames (note: what about acceleration?) as inputs, and outputs two scalars: $Q(s,right)$ and $Q(s,left)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network has three conv/batchnorm/relu layers, followed by a fully-connected output layer\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "                        nn.BatchNorm2d(16))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "                        nn.BatchNorm2d(32))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "                        nn.BatchNorm2d(32))\n",
    "        self.fc_out = nn.Linear(20672, 2) # how do you come up with # of input features?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = x.view(x.size(0), -1) # reshape into single vector\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# screen capture\n",
    "screen_width = 600 # hardcoded in cartpole.py render()\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2 # x_threshold is 2.4, which defines termination state\n",
    "    scale = screen_width/world_width\n",
    "    # return middle of the cart (converts simulation coordinates to display coordinates and shifts origin)\n",
    "    return int(env.state[0] * scale + screen_width / 2.0) # env.state[0] is position (origin is middle of screen)\n",
    "\n",
    "def get_screen():\n",
    "    '''Returns a torch tensor of a square slice of pixels surrounding the cart location'''\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen = screen.transpose(2,0,1) # switch from (height x width x nChannel) to (nChannel, height, width)\n",
    "    screen = screen[:,160:320,:] # remove top and bottom of frame\n",
    "    # remove edges around cart\n",
    "    x_cart = get_cart_location()\n",
    "    # vertical segment width, centered at cart\n",
    "    view_width = 330\n",
    "    x_min = max(x_cart-view_width//2.0, 0)\n",
    "    x_max = min(x_cart+view_width//2.0, screen_width)\n",
    "    screen = screen[:,:,int(x_min):int(x_max)]\n",
    "    \n",
    "    # normalize pixel values and ensure continuous memory\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32)/255\n",
    "    # adds in batch dimension and returns\n",
    "    # coverts to torch tensor and adds batch dimension\n",
    "    return torch.from_numpy(np.expand_dims(screen, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate policy network to train and replay memory\n",
    "policy_net = DQN()\n",
    "memory = ReplayMemory(capacity=10000)\n",
    "\n",
    "epsilon = 0.9\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "# used trained DQN to choose action based on state\n",
    "# to start with, use fixed epsilon (for epsilon-greedy policy)\n",
    "def select_action(state):\n",
    "    '''Selects an action (left=0, right=1) based on DQN with state as input'''\n",
    "    with torch.no_grad():\n",
    "        # predict action values\n",
    "        actions = policy_net(state)\n",
    "    \n",
    "    if(np.random.uniform(0,1) < 0.9):\n",
    "#         return actions.index(max(actions))\n",
    "        return actions.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(np.random.choice([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "gamma = 0.999\n",
    "\n",
    "def optimize_step():\n",
    "    # sample randomly from stored (state, action, reward, new_state) sets\n",
    "    # don't start optimizing until a minimum number of transitions have been taken\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size) # returns batch_size tuples (state, action, new_state, reward)\n",
    "    batch = Transition(*zip(*transitions)) # reshuffles to 4 tuples of batch_size elements\n",
    "    \n",
    "    # create masks of final and non-final states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, \n",
    "                                            batch.new_state)), dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.new_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    # extract elements of batch\n",
    "    # cat takes tuple of tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.tensor(batch.action, dtype=torch.uint8) # complains that it got a tuple when expecting a Variable\n",
    "    reward_batch = torch.tensor(batch.reward, dtype=torch.uint8)\n",
    "    \n",
    "    # calculate Q(s,a), the state-action value function which is being approximated by the DQN\n",
    "    # size mismatch. m1: [100 x 20672], m2: [448 x 2] --> this is at fully connected layer\n",
    "    tmp = policy_net(state_batch)\n",
    "    state_action_values = tmp.gather(1, action_batch)\n",
    "    \n",
    "    # calculate V(s'), the value of all subsequent states\n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "didaloop\n",
      "memory at  100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.ByteTensor for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1b15845a32f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# optimize network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moptimize_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode %i of %i done'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-fe06d25fc5bb>\u001b[0m in \u001b[0;36moptimize_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# size mismatch. m1: [100 x 20672], m2: [448 x 2] --> this is at fully connected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# calculate V(s'), the value of all subsequent states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.ByteTensor for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "# init environment and state\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "n_episodes = 10\n",
    "for i_episode in range(n_episodes):\n",
    "    env.reset()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - current_screen\n",
    "\n",
    "    # loop until the episode finishes\n",
    "    init_loop = True\n",
    "    done = False\n",
    "    while not done:\n",
    "        print('didaloop')\n",
    "        # select and perform an action\n",
    "        # on first loop, randomly choose action, since state is undefined (no previous state)\n",
    "        if(init_loop):\n",
    "            action = np.random.choice([0,1])\n",
    "        else:\n",
    "            # perform forward pass through DQN\n",
    "            action = select_action(state)\n",
    "        \n",
    "        # update environment based on action taken\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # observe resulting state (if episode hasn't terminated)\n",
    "        # state is difference between previous and current screen image\n",
    "        next_state = None\n",
    "        if(not done):\n",
    "            prev_screen = current_screen\n",
    "            current_screen = get_screen()\n",
    "            next_state = current_screen - prev_screen\n",
    "\n",
    "        # store the new transition to memory\n",
    "        print('memory at ', len(memory))\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # update the state\n",
    "        state = next_state\n",
    "\n",
    "        # optimize network\n",
    "        optimize_step()\n",
    "    \n",
    "    print('episode %i of %i done' %(i_episode+1, n_episodes))\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate success of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "env.reset()\n",
    "\n",
    "fig = plt.figure()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "for _ in range(200):\n",
    "    x = env.render(mode='rgb_array') # in rgb_array mode this returns a numpy array\n",
    "    \n",
    "    # update the figure to show new state\n",
    "    img.set_data(x) # this is a lot faster than calling imshow every time\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if(done): # it looks like the failure condition is quite a small deviation from vertical (it's 12deg)\n",
    "        print(done)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
