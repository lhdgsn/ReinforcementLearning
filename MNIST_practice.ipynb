{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://github.com/pytorch/examples/blob/master/mnist/main.py for an example\n",
    "\n",
    "Things to figure out:\n",
    "* weight initializations (use https://pytorch.org/docs/master/nn.html#torch-nn-init)\n",
    "* live graphing of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.tensor as T\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root='./mnist_data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=4, shuffle=True)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='./mnist_data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn methods\n",
    "Convolution layers https://pytorch.org/docs/stable/nn.html?#convolution-layers\n",
    "\n",
    "Pooling layers https://pytorch.org/docs/stable/nn.html?#pooling-layers\n",
    "\n",
    "Padding layers https://pytorch.org/docs/stable/nn.html?#padding-layers\n",
    "\n",
    "Non-linear activations https://pytorch.org/docs/stable/nn.html?#non-linear-activations-weighted-sum-nonlinearity\n",
    "\n",
    "Normalization layers https://pytorch.org/docs/stable/nn.html?#normalization-layers\n",
    "\n",
    "Recurrent layers https://pytorch.org/docs/stable/nn.html?#normalization-layers\n",
    "\n",
    "Linear layers https://pytorch.org/docs/stable/nn.html?#linear-layers\n",
    "\n",
    "Dropout layers https://pytorch.org/docs/stable/nn.html?#dropout-layers\n",
    "\n",
    "Loss functions https://pytorch.org/docs/stable/nn.html?#loss-functions\n",
    "\n",
    "Vision layers https://pytorch.org/docs/stable/nn.html?#vision-layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "class mnist_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 20, kernel_size=7, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(6*6*20, 121)\n",
    "        self.fc2 = nn.Linear(121, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "loss: 0.784215\n",
      "loss: 0.188390\n",
      "loss: 0.163224\n",
      "loss: 0.125645\n",
      "loss: 0.106898\n",
      "loss: 0.088044\n",
      "loss: 0.097614\n",
      "Training epoch: 2\n",
      "loss: 0.074930\n",
      "loss: 0.087394\n",
      "loss: 0.070652\n",
      "loss: 0.068146\n",
      "loss: 0.062035\n",
      "loss: 0.067421\n",
      "loss: 0.067272\n",
      "Training epoch: 3\n",
      "loss: 0.051382\n",
      "loss: 0.048713\n",
      "loss: 0.060203\n",
      "loss: 0.046884\n",
      "loss: 0.058429\n",
      "loss: 0.045207\n",
      "loss: 0.047194\n",
      "Training epoch: 4\n",
      "loss: 0.037979\n",
      "loss: 0.040030\n",
      "loss: 0.036117\n",
      "loss: 0.044277\n",
      "loss: 0.040929\n",
      "loss: 0.045510\n",
      "loss: 0.041777\n",
      "Training epoch: 5\n",
      "loss: 0.034922\n",
      "loss: 0.034496\n",
      "loss: 0.029339\n",
      "loss: 0.035563\n",
      "loss: 0.039827\n",
      "loss: 0.027901\n",
      "loss: 0.036755\n",
      "Training epoch: 6\n",
      "loss: 0.027691\n",
      "loss: 0.030956\n",
      "loss: 0.033826\n",
      "loss: 0.026702\n",
      "loss: 0.026709\n",
      "loss: 0.027927\n",
      "loss: 0.036674\n",
      "Training epoch: 7\n",
      "loss: 0.022913\n",
      "loss: 0.029878\n",
      "loss: 0.025263\n",
      "loss: 0.027349\n",
      "loss: 0.026655\n",
      "loss: 0.024728\n",
      "loss: 0.026758\n",
      "Training epoch: 8\n",
      "loss: 0.021379\n",
      "loss: 0.015413\n",
      "loss: 0.020381\n",
      "loss: 0.020706\n",
      "loss: 0.022676\n",
      "loss: 0.030986\n",
      "loss: 0.023343\n",
      "Training epoch: 9\n",
      "loss: 0.014914\n",
      "loss: 0.015784\n",
      "loss: 0.018038\n",
      "loss: 0.017237\n",
      "loss: 0.018960\n",
      "loss: 0.018887\n",
      "loss: 0.026003\n",
      "Training epoch: 10\n",
      "loss: 0.013646\n",
      "loss: 0.015101\n",
      "loss: 0.014421\n",
      "loss: 0.018799\n",
      "loss: 0.017198\n",
      "loss: 0.018516\n",
      "loss: 0.020759\n",
      "Training epoch: 11\n",
      "loss: 0.011103\n",
      "loss: 0.008462\n",
      "loss: 0.013139\n",
      "loss: 0.011451\n",
      "loss: 0.021050\n",
      "loss: 0.017244\n",
      "loss: 0.011546\n",
      "Training epoch: 12\n",
      "loss: 0.008156\n",
      "loss: 0.010911\n",
      "loss: 0.011336\n",
      "loss: 0.009356\n",
      "loss: 0.012210\n",
      "loss: 0.016202\n",
      "loss: 0.016441\n",
      "Training epoch: 13\n",
      "loss: 0.010585\n",
      "loss: 0.007124\n",
      "loss: 0.011587\n",
      "loss: 0.008991\n",
      "loss: 0.011258\n",
      "loss: 0.011941\n",
      "loss: 0.011328\n",
      "Training epoch: 14\n",
      "loss: 0.006372\n",
      "loss: 0.010331\n",
      "loss: 0.008072\n",
      "loss: 0.008826\n",
      "loss: 0.009288\n",
      "loss: 0.008299\n",
      "loss: 0.013433\n",
      "Training epoch: 15\n",
      "loss: 0.007478\n",
      "loss: 0.006679\n",
      "loss: 0.008265\n",
      "loss: 0.011576\n",
      "loss: 0.006966\n",
      "loss: 0.013289\n",
      "loss: 0.010237\n",
      "Training epoch: 16\n",
      "loss: 0.009727\n",
      "loss: 0.003841\n",
      "loss: 0.006565\n",
      "loss: 0.006813\n",
      "loss: 0.005747\n",
      "loss: 0.006607\n",
      "loss: 0.006538\n",
      "Training epoch: 17\n",
      "loss: 0.005912\n",
      "loss: 0.003563\n",
      "loss: 0.003168\n",
      "loss: 0.005091\n",
      "loss: 0.007331\n",
      "loss: 0.005652\n",
      "loss: 0.007940\n",
      "Training epoch: 18\n",
      "loss: 0.002667\n",
      "loss: 0.001973\n",
      "loss: 0.006336\n",
      "loss: 0.005215\n",
      "loss: 0.004616\n",
      "loss: 0.004868\n",
      "loss: 0.007156\n",
      "Training epoch: 19\n",
      "loss: 0.004509\n",
      "loss: 0.003067\n",
      "loss: 0.004099\n",
      "loss: 0.004061\n",
      "loss: 0.003385\n",
      "loss: 0.007833\n",
      "loss: 0.006178\n",
      "Training epoch: 20\n",
      "loss: 0.004919\n",
      "loss: 0.002858\n",
      "loss: 0.001619\n",
      "loss: 0.003123\n",
      "loss: 0.003498\n",
      "loss: 0.002800\n",
      "loss: 0.004030\n",
      "Training epoch: 21\n",
      "loss: 0.004111\n",
      "loss: 0.003289\n",
      "loss: 0.001869\n",
      "loss: 0.001828\n",
      "loss: 0.001579\n",
      "loss: 0.002827\n",
      "loss: 0.003542\n",
      "Training epoch: 22\n",
      "loss: 0.003042\n",
      "loss: 0.004513\n",
      "loss: 0.002772\n",
      "loss: 0.001628\n",
      "loss: 0.003074\n",
      "loss: 0.003857\n",
      "loss: 0.003385\n",
      "Training epoch: 23\n",
      "loss: 0.000727\n",
      "loss: 0.000768\n",
      "loss: 0.000769\n",
      "loss: 0.001108\n",
      "loss: 0.001887\n",
      "loss: 0.002504\n",
      "loss: 0.002008\n",
      "Training epoch: 24\n",
      "loss: 0.000803\n",
      "loss: 0.001392\n",
      "loss: 0.000860\n",
      "loss: 0.002421\n",
      "loss: 0.001460\n",
      "loss: 0.001078\n",
      "loss: 0.001359\n",
      "Training epoch: 25\n",
      "loss: 0.000784\n",
      "loss: 0.000296\n",
      "loss: 0.001023\n",
      "loss: 0.000627\n",
      "loss: 0.001287\n",
      "loss: 0.000429\n",
      "loss: 0.000806\n",
      "Training epoch: 26\n",
      "loss: 0.000317\n",
      "loss: 0.000641\n",
      "loss: 0.000297\n",
      "loss: 0.000347\n",
      "loss: 0.000268\n",
      "loss: 0.000225\n",
      "loss: 0.000613\n",
      "Training epoch: 27\n",
      "loss: 0.000137\n",
      "loss: 0.000862\n",
      "loss: 0.000226\n",
      "loss: 0.000292\n",
      "loss: 0.000164\n",
      "loss: 0.000188\n",
      "loss: 0.000633\n",
      "Training epoch: 28\n",
      "loss: 0.000142\n",
      "loss: 0.000107\n",
      "loss: 0.000282\n",
      "loss: 0.000695\n",
      "loss: 0.000252\n",
      "loss: 0.000233\n",
      "loss: 0.000142\n",
      "Training epoch: 29\n",
      "loss: 0.000192\n",
      "loss: 0.000146\n",
      "loss: 0.000184\n",
      "loss: 0.000140\n",
      "loss: 0.000128\n",
      "loss: 0.000096\n",
      "loss: 0.000104\n",
      "Training epoch: 30\n",
      "loss: 0.000100\n",
      "loss: 0.000104\n",
      "loss: 0.000109\n",
      "loss: 0.000108\n",
      "loss: 0.000100\n",
      "loss: 0.000113\n",
      "loss: 0.000089\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "n_epochs = 5\n",
    "\n",
    "net = mnist_net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    print('Training epoch: ' + str(e+1))\n",
    "    L = 0\n",
    "    for n, (im, im_label) in enumerate(train_loader):        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass through network\n",
    "        output = net(im)\n",
    "\n",
    "        loss = criterion(output, im_label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        L += loss.item()\n",
    "        if(n%2000 == 1999):\n",
    "            print('loss: %f' %(L/2000))\n",
    "            L = 0\n",
    "\n",
    "print('Done training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified 99.2% of digits\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance on test set\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "with torch.no_grad():\n",
    "    for im, im_label in test_loader:\n",
    "        # predict using trained net\n",
    "        out = net(im)\n",
    "        \n",
    "        # check correct predictions\n",
    "        n_correct += torch.sum(im_label == out.max(dim=1)[1])\n",
    "        n_total += 4.0\n",
    "        \n",
    "print('Correctly classified %.1f%% of digits' %(100*n_correct.item()/n_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
